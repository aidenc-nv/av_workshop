{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "976a1c53",
   "metadata": {},
   "source": [
    "# Milestone Workshop: Cosmos Transfer 2.5\n",
    "**Authors:** Aiden Chang, Akul Santhosh\n",
    "\n",
    "\n",
    "This notebook is a hands on guide for Milestone data. The goal is for you to understand, create, and use the multi-control modalities that power Cosmos Transfer 2.5 (CT 2.5).\n",
    "\n",
    "**Important** Select the Cosmos Transfer 2.5 Kernel\n",
    "\n",
    "We will cover:\n",
    "\n",
    "Let's begin by setting up our environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8744fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli login' is deprecated. Use 'hf auth login' instead.\u001b[0m\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `hf`CLI if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `read_token` has been saved to /home/nvidia/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /home/nvidia/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `read_token`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token \"YOUR TOKEN HERE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687bfbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"prompts\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "os.makedirs(\"control_modalities\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb370b4e",
   "metadata": {},
   "source": [
    "## Key Concepts: Governing Strength\n",
    "Success with CT 2.5 depends on balancing two key principles: the text prompt's influence (Guidance Scale) and the influence of the visual controls (Control Weight Normalization).\n",
    "\n",
    "### 1.1. Guidance Scale (Prompt Strength)\n",
    "This dictates how strictly the model adheres to your text prompt versus the visual controls.\n",
    "- What it is: Controls the influence of the text prompt.\n",
    "- Good Starting Point: Guidance = 3.\n",
    "- When to Increase: Increase to 5+ if the visual output fails to incorporate the changes described in your prompt (e.g., trying to change a shirt from \"blue\" to \"red\").\n",
    "\n",
    "### 1.2. Control Weight Normalization \n",
    "This governs how the model balances multiple control modalities (e.g., Edge + Seg + Vis) against each other.\n",
    "- Rule 1: Weights WILL NOT Normalize if the total sum of all control weights is 1.0 or less. The weights are applied as-is.\n",
    "    - Example: {seg: 0.2, edge: 0.2} (sum 0.4) will be used as-is.\n",
    "- Rule 2: Weights WILL NORMALIZE if the total sum is greater than 1.0. The weights are re-scaled proportionally so the new total sum equals 1.0.\n",
    "    - Example: {seg: 4.0, edge: 1.0} (sum 5.0) will be normalized and run as {seg: 0.8, edge: 0.2}."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b24d265",
   "metadata": {},
   "source": [
    "## 2. Technical Details: The Control Modalities\n",
    "The system uses modalities to inject structural, semantic, relative, and visual consistency into the video. We will generate these control modalities using the following video:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178ebc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"600\" controls>\n",
       "  <source src=\"milestone_data/clip_0_harder_version.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import sys\n",
    "\n",
    "milestone_example = \"milestone_data/clip_0_harder_version.mp4\"\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"{milestone_example}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1164f",
   "metadata": {},
   "source": [
    "### 2.1. Edge Control (Structure Preservation)\n",
    "\n",
    "Function: Preserves the original structure, shape, and layout of the video.\n",
    "\n",
    "Edge control is natively supported in CT2.5. However, when object and background contours are visually similar, the default Canny-based edge detection may miss important boundaries. In these situations, it’s helpful to run a preprocessing step within the CT2.5 repository to generate a cleaner, higher-contrast edge map.\n",
    "\n",
    "Hands-On: Let's generate our own edge-control video with enhanced contrast and brightness. We can do this using the command line or by using the CT2.5 python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a971731d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished generating the new video!\n"
     ]
    }
   ],
   "source": [
    "from cosmos_transfer2_5.cosmos_transfer2._src.transfer2.auxiliary.utils.generate_edges import generate_edges\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_output_path(file_path, modality_type):\n",
    "    name = Path(file_path).stem\n",
    "    parent_dir = os.path.join(\"control_modalities\", name)\n",
    "    os.makedirs(parent_dir, exist_ok=True)\n",
    "    return os.path.join(parent_dir, f\"{modality_type}.mp4\")\n",
    "\n",
    "# --- KEY PARAMETERS ---\n",
    "# We increase brightness to help distinguish contours\n",
    "bright = 1\n",
    "contrast = 0.2\n",
    "\n",
    "# --- EXERCISE ---\n",
    "in_path = milestone_example\n",
    "out_path = generate_output_path(milestone_example, \"edge\")\n",
    "\n",
    "generate_edges(in_path, out_path, bright=bright, contrast=contrast)\n",
    "\n",
    "print(\"finished generating the new video!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdbeb08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"600\" controls>\n",
       "  <source src=\"control_modalities/clip_0_harder_version/edge_h264.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ffmpeg -y -i control_modalities/clip_0_harder_version/edge.mp4 -vcodec libx264 -acodec aac control_modalities/clip_0_harder_version/edge_h264.mp4 -v quiet\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"control_modalities/clip_0_harder_version/edge_h264.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1286512d",
   "metadata": {},
   "source": [
    "### 2.2. Segmentation (Seg) Control (Structural Change & Semantic Replacement)\n",
    "\n",
    "**Function**: Facilitates large, structural changes and semantic replacement. Used to completely transform or replace objects, people, or backgrounds. \n",
    "\n",
    "There are three parts:\n",
    "1. **Identify objects in the scene:** Use an object detection model (e.g., [RAM++](https://github.com/xinyu1205/recognize-anything)) to obtain object labels and bounding boxes. *(We skip this step here, as multiple approaches can be applied.)*\n",
    "2. **Prompt the objects:** Detection models such as [Grounding Dino](https://github.com/IDEA-Research/GroundingDINO) or [YOLO](https://docs.ultralytics.com/models/yolov9/) can generate either box or point prompts (e.g., coordinates). These prompts guide the segmentation process. *(If a standalone detector provides both labels and spatial prompts, Step 1 is implicitly covered.)*\n",
    "3. **Generate pixel-accurate segmentations:** Feed the prompts (boxes or points) into SAM/SAM2 to obtain high-quality masks that drive the structural or semantic edits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269558d",
   "metadata": {},
   "source": [
    "In CT2.5, step 2 and 3 are squished together. You can use the following command to automatically segment the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f35a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/sam3.git\n",
      "  Cloning https://github.com/facebookresearch/sam3.git to /tmp/pip-req-build-pzz8ts5t\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/sam3.git /tmp/pip-req-build-pzz8ts5t\n",
      "  Resolved https://github.com/facebookresearch/sam3.git to commit 757bbb0206a0b68bee81b17d7eb4877177025b2f\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: timm>=1.0.17 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from sam3==0.1.0) (1.0.21)\n",
      "Collecting numpy==1.26 (from sam3==0.1.0)\n",
      "  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Requirement already satisfied: tqdm in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from sam3==0.1.0) (4.67.1)\n",
      "Collecting ftfy==6.1.1 (from sam3==0.1.0)\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: regex in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from sam3==0.1.0) (2025.10.23)\n",
      "Requirement already satisfied: iopath>=0.1.10 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from sam3==0.1.0) (0.1.10)\n",
      "Requirement already satisfied: typing_extensions in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from sam3==0.1.0) (4.15.0)\n",
      "Requirement already satisfied: huggingface_hub in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from sam3==0.1.0) (0.36.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from ftfy==6.1.1->sam3==0.1.0) (0.2.14)\n",
      "Requirement already satisfied: portalocker in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from iopath>=0.1.10->sam3==0.1.0) (3.2.0)\n",
      "Requirement already satisfied: torch in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from timm>=1.0.17->sam3==0.1.0) (2.7.1+cu128)\n",
      "Requirement already satisfied: torchvision in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from timm>=1.0.17->sam3==0.1.0) (0.22.1+cu128)\n",
      "Requirement already satisfied: pyyaml in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from timm>=1.0.17->sam3==0.1.0) (6.0.3)\n",
      "Requirement already satisfied: safetensors in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from timm>=1.0.17->sam3==0.1.0) (0.6.2)\n",
      "Requirement already satisfied: filelock in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from huggingface_hub->sam3==0.1.0) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from huggingface_hub->sam3==0.1.0) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from huggingface_hub->sam3==0.1.0) (25.0)\n",
      "Requirement already satisfied: requests in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from huggingface_hub->sam3==0.1.0) (2.32.5)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from huggingface_hub->sam3==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->sam3==0.1.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->sam3==0.1.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->sam3==0.1.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from requests->huggingface_hub->sam3==0.1.0) (2025.10.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.1 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torch->timm>=1.0.17->sam3==0.1.0) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from triton==3.3.1->torch->timm>=1.0.17->sam3==0.1.0) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch->timm>=1.0.17->sam3==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from jinja2->torch->timm>=1.0.17->sam3==0.1.0) (3.0.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./cosmos_transfer2_5/.venv/lib/python3.10/site-packages (from torchvision->timm>=1.0.17->sam3==0.1.0) (11.3.0)\n",
      "Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sam3\n",
      "  Building wheel for sam3 (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sam3: filename=sam3-0.1.0-py3-none-any.whl size=183935 sha256=0dea80239f5b9f9c10bb576412e960c54858930f64fb41fc0bf46bad39c4db5f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wccpl_eq/wheels/1f/ac/85/ba4e79ab72069e403df4f17bfcad8f1de3f613a11af33d4001\n",
      "Successfully built sam3\n",
      "Installing collected packages: numpy, ftfy, sam3\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.2.6\n",
      "\u001b[2K    Uninstalling numpy-2.2.6:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[2K  Attempting uninstall: ftfy━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: ftfy 6.3.1━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling ftfy-6.3.1:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled ftfy-6.3.1━━━\u001b[0m \u001b[32m0/3\u001b[0m [numpy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [sam3][32m2/3\u001b[0m [sam3]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cosmos-oss 1.3.3 requires ftfy>=6.3.1, but you have ftfy 6.1.1 which is incompatible.\n",
      "cosmos-transfer2 1.3.3 requires cosmos-oss==0.1.0, but you have cosmos-oss 1.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed ftfy-6.1.1 numpy-1.26.0 sam3-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install 'git+https://github.com/facebookresearch/sam3.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also specify the tags or use models such as RAM++ to detect them. \n",
    "# NOTE: The tags must be lowercase and separated by a period! \n",
    "!{sys.executable} cosmos_transfer2_5/cosmos_transfer2/_src/transfer2/auxiliary/sam2/sam2_pipeline.py \\\n",
    "    --input_video assets/wave.mp4 \\\n",
    "    --output_video assets/seg.mp4 \\\n",
    "    --output_tensor assets/tensor.pt \\\n",
    "    --mode prompt \\\n",
    "    --prompt \"floor. ceiling. wall. staircase. railing. bench. light. person. man. shirt. pants. shoes. badge. hand. arm. wave. pose. hall. atrium. building. door. corridor. background. reflection. shadow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92e326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"600\" controls>\n",
       "  <source src=\"assets/seg_h264.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!ffmpeg -y -i assets/seg.mp4 -vcodec libx264 -acodec aac assets/seg_h264.mp4 -v quiet\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"assets/seg_h264.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f421d",
   "metadata": {},
   "source": [
    "### 2.3. Vis Control (Lighting & Background Feel)\n",
    "Function: Preserves the original video’s background, lighting, and overall appearance. It applies a subtle smoothing/blur. Vis control is natively built into the CT 2.5 repository. No need to pre-generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cb4e2",
   "metadata": {},
   "source": [
    "### 2.4. Depth Control (Distance Consistancy)\n",
    "Function: Preserves the original video’s 3D Geometry Depth control is natively built into the CT 2.5 repository. No need to pre-generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd366a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cosmos_transfer2._src.transfer2.auxiliary.depth_anything.video_depth_anything import VideoDepthAnythingModel\n",
    "\n",
    "# depth_model = VideoDepthAnythingModel()\n",
    "# depth_model.setup()\n",
    "# depth_maps = model.generate(video_np)\n",
    "# depth_tensor = torch.from_numpy(depth_maps.astype(np.float32))\n",
    "# d_min, d_max = depth_tensor.min(), depth_tensor.max()\n",
    "# depth_normalized = (depth_tensor - d_min) / (d_max - d_min + 1e-8) * 255.0\n",
    "# depth_normalized = depth_normalized.unsqueeze(0)  # (1, T, H, W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb794a",
   "metadata": {},
   "source": [
    "## 3. Hands-On: Multi-Control Recipes\n",
    "| Task | Suggested Controls & Settings| \n",
    "|--|--|\n",
    "|Change clothing or textures|Edge: 1.0, Guidance: 3|\n",
    "|Change lighting|Edge: 1.0 + Vis: 0.2, Guidance: 3|\n",
    "|Change background, keep subject|Filtered Edge: 1.0 + Seg (Mask Inverted): 0.4 + Vis: 0.6, Guidance: 3|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa9478d",
   "metadata": {},
   "source": [
    "### Recipe 1: Color/Texture Change\n",
    "\n",
    "Goal: Modify the color of the person's shirt. This is the simplest recipe. We already generated the edge modality from the previous steps!\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"color_change\",\n",
    "  \"prompt_path\": \"prompts/prompt_color.txt\",\n",
    "  \"video_path\": \"assets/wave.mp4\",\n",
    "  \"guidance\": 3,\n",
    "  \"edge\": {\n",
    "    \"control_weight\": 1.0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "*Note: Why no Vis? Vis would preserve the original colors. We rely only on Edge (1.0) to hold the shape and let the Prompt do all the color work.*\n",
    "\n",
    "#### Recipe:\n",
    "<img src=\"assets/color_change_recipe.png\" width=\"300\"/>\n",
    "\n",
    "#### Example Results:\n",
    "<div style=\"display: flex; gap: 20px;\">\n",
    "  <video src=\"assets/wave.mp4\" width=\"45%\" controls></video>\n",
    "  <video src=\"assets/color.mp4\" width=\"45%\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9892a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change the prompt\n",
    "prompt = \"The camera pans over the person sitting on a chair wearing a red t-shirt\"\n",
    "with open(\"temp/background_change.txt\", \"w\") as f:\n",
    "    f.write(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574a2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1119 04:58:17.562000 1570167 torch/distributed/run.py:766] \n",
      "W1119 04:58:17.562000 1570167 torch/distributed/run.py:766] *****************************************\n",
      "W1119 04:58:17.562000 1570167 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1119 04:58:17.562000 1570167 torch/distributed/run.py:766] *****************************************\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 4529.92it/s]\n",
      "[\u001b[32m11-19 04:58:26\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:171:path\u001b[0m] Downloading checkpoint nvidia/Cosmos-Transfer2.5-2B/general/edge(ecd0ba00-d598-4f94-aa09-e8627899c431)\n",
      "[\u001b[32m11-19 04:58:26\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:96:_download\u001b[0m] Downloading checkpoint file from Hugging Face with {'repo_id': 'nvidia/Cosmos-Transfer2.5-2B', 'repo_type': 'model', 'revision': 'bd963eabcfc2d61dc4ea365cacf41d45ac480aa5', 'filename': 'general/edge/ecd0ba00-d598-4f94-aa09-e8627899c431_ema_bf16.pt'}\n",
      "[\u001b[32m11-19 04:58:26\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/packages/cosmos-oss/cosmos_oss/init.py:96:_init_log_files\u001b[0m] Log saved to outputs/temp/console.log\n",
      "Fetching 102 files:   0%|                               | 0/102 [00:00<?, ?it/s][\u001b[32m11-19 04:58:26\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:171:path\u001b[0m] Downloading checkpoint nvidia/Cosmos-Guardrail1(9c7b7da4-2d95-45bb-9cb8-2eed954e9736)\n",
      "[\u001b[32m11-19 04:58:26\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:131:_download\u001b[0m] Downloading checkpoint from Hugging Face with {'repo_id': 'nvidia/Cosmos-Guardrail1', 'repo_type': 'model', 'revision': 'd6d4bfa899a71454a700907664f3e88f503950cf', 'allow_patterns': ['*']}\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 1406.42it/s]\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 1488.44it/s]\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 1484.21it/s]\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 1811.05it/s]\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 1731.41it/s]\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 1801.86it/s]\n",
      "Fetching 102 files: 100%|███████████████████| 102/102 [00:00<00:00, 1114.99it/s]\n",
      "[\u001b[32m11-19 04:58:40\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:171:path\u001b[0m] Downloading checkpoint nvidia/Cosmos-Transfer2.5-2B/general/blur(20d9fd0b-af4c-4cca-ad0b-f9b45f0805f1)\n",
      "[\u001b[32m11-19 04:58:40\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:96:_download\u001b[0m] Downloading checkpoint file from Hugging Face with {'repo_id': 'nvidia/Cosmos-Transfer2.5-2B', 'repo_type': 'model', 'revision': 'bd963eabcfc2d61dc4ea365cacf41d45ac480aa5', 'filename': 'general/blur/20d9fd0b-af4c-4cca-ad0b-f9b45f0805f1_ema_bf16.pt'}\n",
      "[\u001b[32m11-19 04:58:53\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:171:path\u001b[0m] Downloading checkpoint Wan2.1/vae(685afcaa-4de2-42fe-b7b9-69f7a2dee4d8)\n",
      "[\u001b[32m11-19 04:58:53\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:96:_download\u001b[0m] Downloading checkpoint file from Hugging Face with {'repo_id': 'nvidia/Cosmos-Predict2.5-2B', 'repo_type': 'model', 'revision': '6787e176dce74a101d922174a95dba29fa5f0c55', 'filename': 'tokenizer.pth'}\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 9451.54it/s]\n",
      "[\u001b[32m11-19 04:58:56\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:171:path\u001b[0m] Downloading checkpoint Qwen/Qwen2.5-VL-7B-Instruct(7219c6c7-f878-4137-bbdb-76842ea85e70)\n",
      "[\u001b[32m11-19 04:58:56\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:131:_download\u001b[0m] Downloading checkpoint from Hugging Face with {'repo_id': 'nvidia/Cosmos-Reason1-7B', 'repo_type': 'model', 'revision': '3210bec0495fdc7a8d3dbb8d58da5711eab4b423', 'allow_patterns': ['*']}\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 14720.83it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 31104.37it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 19634.84it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 15929.29it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 11289.02it/s]\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 7118.27it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 21880.40it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 24083.90it/s]\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 7526.01it/s]\n",
      "[\u001b[32m11-19 04:58:58\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:171:path\u001b[0m] Downloading checkpoint nvidia/Cosmos-Reason1.1-7B(cb3e3ffa-7b08-4c34-822d-61c7aa31a14f)\n",
      "[\u001b[32m11-19 04:58:58\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/_src/imaginaire/utils/checkpoint_db.py:131:_download\u001b[0m] Downloading checkpoint from Hugging Face with {'repo_id': 'nvidia/Cosmos-Reason1-7B', 'repo_type': 'model', 'revision': '3210bec0495fdc7a8d3dbb8d58da5711eab4b423', 'allow_patterns': ['*']}\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 5434.12it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 36792.14it/s]\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 5780.34it/s]\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 5159.05it/s]\n",
      "Fetching 13 files: 100%|██████████████████████| 13/13 [00:00<00:00, 6243.67it/s]\n",
      "Fetching 13 files: 100%|█████████████████████| 13/13 [00:00<00:00, 14720.83it/s]\n",
      "[\u001b[32m11-19 04:59:41\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:102:__init__\u001b[0m] Found 2 hint keys across all samples\n",
      "[\u001b[32m11-19 04:59:41\u001b[0m|\u001b[33m\u001b[1mWARNING\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:104:__init__\u001b[0m] Loading the multicontrol model. Multicontrol inference is not strictly equal to single control\n",
      "[\u001b[32m11-19 04:59:41\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:112:__init__\u001b[0m] Saved config to outputs/temp/config.yaml\n",
      "[\u001b[32m11-19 04:59:41\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:120:generate\u001b[0m] Generating 1 samples: ['temp']\n",
      "[\u001b[32m11-19 04:59:41\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:124:generate\u001b[0m] [1/1] Processing sample temp\n",
      "[\u001b[32m11-19 04:59:41\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:152:_generate_sample\u001b[0m] Saved arguments to outputs/temp/temp.json\n",
      "[\u001b[32m11-19 04:59:41\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:156:_generate_sample\u001b[0m] Running guardrail check on prompt...\n",
      "[\u001b[32m11-19 04:59:45\u001b[0m|\u001b[32m\u001b[1mSUCCESS\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:166:_generate_sample\u001b[0m] Passed guardrail on prompt\n",
      "[\u001b[32m11-19 04:59:48\u001b[0m|\u001b[32m\u001b[1mSUCCESS\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:179:_generate_sample\u001b[0m] Passed guardrail on negative prompt\n",
      "[\u001b[32m11-19 04:59:48\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:184:_generate_sample\u001b[0m] Processing the following paths: {'edge': '/home/nvidia/workspace/cosmos-robotics-sa-training/temp/edge_output.mp4', 'edge_mask': None, 'edge_mask_prompt': None, 'vis': '/home/nvidia/workspace/cosmos-robotics-sa-training/temp/output.mp4', 'vis_mask': '/home/nvidia/workspace/cosmos-robotics-sa-training/temp/sam2_pan_reverse_mask.mp4', 'vis_mask_prompt': None}\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:19<00:00,  4.00s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:19<00:00,  4.00s/it]\n",
      "\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:19<00:00,  4.00s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:19<00:00,  4.00s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:19<00:00,  4.00s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:19<00:00,  4.00s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:19<00:00,  4.00s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:18<00:00,  3.95s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:18<00:00,  3.95s/it]\n",
      "\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:18<00:00,  3.95s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:18<00:00,  3.95s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:18<00:00,  3.95s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:18<00:00,  3.95s/it]\n",
      "Generating samples: 100%|███████████████████████| 35/35 [02:18<00:00,  3.95s/it]\n",
      "[\u001b[32m11-19 05:05:51\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:238:_generate_sample\u001b[0m] edge control video saved to outputs/temp/temp_control_edge.mp4\n",
      "[\u001b[32m11-19 05:05:55\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:238:_generate_sample\u001b[0m] vis control video saved to outputs/temp/temp_control_vis.mp4\n",
      "[\u001b[32m11-19 05:05:55\u001b[0m|\u001b[1mINFO\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:242:_generate_sample\u001b[0m] Running guardrail check on video...\n",
      "[\u001b[32m11-19 05:06:06\u001b[0m|\u001b[32m\u001b[1mSUCCESS\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:252:_generate_sample\u001b[0m] Passed guardrail on generated video\n",
      "[\u001b[32m11-19 05:06:12\u001b[0m|\u001b[32m\u001b[1mSUCCESS\u001b[0m|\u001b[36mcosmos_transfer2_5/cosmos_transfer2/inference.py:266:_generate_sample\u001b[0m] Generated video saved to outputs/temp/temp.mp4\n"
     ]
    }
   ],
   "source": [
    "# Due to time constraints, we have commented out this excersize. Feel free to run it on your own time.\n",
    "\n",
    "# Run CT2.5 \n",
    "# WARNING: This should take a couple of minutes\n",
    "# !{sys.executable} cosmos_transfer2_5/examples/inference.py -i scripts/color_change.jsonl -o outputs/color_change\n",
    "!torchrun --nproc_per_node=8 --master_port=12341 cosmos_transfer2_5/examples/inference.py -i temp/background_change.jsonl -o outputs/temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db1e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"600\" controls>\n",
       "  <source src=\"outputs/color_change/color_change.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"outputs/color_change/color_change.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c559e7",
   "metadata": {},
   "source": [
    "### Recipe 2: Lighting Change\n",
    "\n",
    "Goal: Modify scene lighting (e.g., day to night) while keeping all objects the same. Configuration:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"change_lighting\",\n",
    "  \"prompt_path\": \"prompt_lighting.txt\",\n",
    "  \"video_path\": \"assets/wave.mp4\",\n",
    "  \"guidance\": 3,\n",
    "  \"edge\": {\n",
    "    \"control_weight\": 1.0\n",
    "  },\n",
    "  \"vis\": {\n",
    "    \"control_weight\": 0.2\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We already generated the edge modality from the previous steps, and vis can be computed on the fly.\n",
    "\n",
    "*Note: We use full Edge (1.0) to lock the structure and a tiny bit of Vis (0.2) to maintain realism, but low enough to allow the prompt to change the lighting.*\n",
    "\n",
    "#### Recipe:\n",
    "<img src=\"assets/lighting_change_recipe.png\" width=\"500\"/>\n",
    "\n",
    "#### Example Results:\n",
    "<div style=\"display: flex; gap: 20px;\">\n",
    "  <video src=\"assets/wave.mp4\" width=\"45%\" controls></video>\n",
    "  <video src=\"assets/lighting.mp4\" width=\"45%\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a533738",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A realistic, static full-body shot of a young man standing in the center of a spacious, modern atrium. He has short dark hair and is dressed casually in a dark grey t-shirt, loose black pants, and white sneakers, with an ID badge clipped to his waistband. He faces the camera directly and waves his right hand continuously in a friendly greeting. The surrounding space is bright and open, featuring a high industrial-style ceiling with exposed white beams and large, angular black structural supports. The floor is polished light grey concrete, subtly reflecting the warm, soft afternoon sunlight that pours in from large windows above. The overall lighting has a gentle golden tint, with natural shadows stretching slightly to the side in the way they do during late afternoon. In the background, a mezzanine level with glass railings is visible, along with several modern wooden benches and tables scattered throughout the area.\"\n",
    "with open(\"prompts/prompt_lighting.txt\", \"w\") as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53663ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to time constraints, we have commented out this excersize. Feel free to run it on your own time.\n",
    "\n",
    "# Run CT2.5 \n",
    "# WARNING: This should take a couple of minutes\n",
    "# !{sys.executable} cosmos_transfer2_5/examples/inference.py -i scripts/lighting_change.jsonl -o outputs/lighting_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61bf744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"600\" controls>\n",
       "  <source src=\"outputs/lighting_change/lighting_change.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"outputs/lighting_change/lighting_change.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f77db",
   "metadata": {},
   "source": [
    "### Recipe 3: Background Change\n",
    "\n",
    "Goal: Modify Background while keeping selected objects and/or subjects the same. Configuration:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"change_background\",\n",
    "  \"prompt_path\": \"prompt.txt\",\n",
    "  \"video_path\": \"original.mp4\",\n",
    "  \"guidance\": 3,\n",
    "  \"edge\": {\n",
    "    \"control_weight\": 1.0,\n",
    "    \"control_path\": \"filtered_edge.mp4\"\n",
    "  },\n",
    "  \"seg\": {\n",
    "    \"control_weight\": 0.4,\n",
    "    \"control_path\": \"segmentation.mp4\",\n",
    "    \"mask_path\": \"mask_inverted.mp4\"\n",
    "  },\n",
    "  \"vis\": {\n",
    "    \"control_weight\": 0.4 // Adjust based on use case\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "We have the filtered edge modality to generate:\n",
    "- `filtered_edge.mp4`\n",
    "\n",
    "#### Recipe:\n",
    "<img src=\"assets/background_change_recipe.png\" width=\"80%\"/>\n",
    "\n",
    "#### Example Results:\n",
    "<div style=\"display: flex; gap: 20px;\">\n",
    "  <video src=\"assets/wave.mp4\" width=\"45%\" controls></video>\n",
    "  <video src=\"assets/ocean.mp4\" width=\"45%\" controls></video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb435b",
   "metadata": {},
   "source": [
    "#### Generating the filtered edge\n",
    "Lets learn how to generate the filtered edge. We found that in practice, this works the best. We generated a mask from the previous step, and we already have our edge!\n",
    "\n",
    "\n",
    "<img src=\"assets/filtered_edge_recipe.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} cosmos_transfer2_5/cosmos_transfer2/_src/transfer2/auxiliary/utils/filter_edges.py \\\n",
    "    assets/edge.mp4 \\\n",
    "    assets/mask.mp4 \\\n",
    "    assets/filtered_edge.mp4 \\\n",
    "    --threshold 0 \\\n",
    "    --grow_px 3 \\\n",
    "    --close_px 3 \\\n",
    "    --feather_px 2\n",
    "\n",
    "# Encoding in a format we can view\n",
    "!ffmpeg -y -i assets/filtered_edge.mp4 -vcodec libx264 -acodec aac assets/filtered_edge_h264.mp4 -v quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d8fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"600\" controls>\n",
       "  <source src=\"assets/filtered_edge_h264.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"assets/filtered_edge_h264.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f648f7",
   "metadata": {},
   "source": [
    "#### Changing the background\n",
    "We have all four ingredients (Reverse mask, Filtered Edge, Seg, and Vis)! Let's now modify our background. \n",
    "\n",
    "*Note: Remember, vis is computed on the fly!*\n",
    "\n",
    "Vis is a modality that can be tuned here depending on your background. Keeping vis generates a clearer background, but may backfire depending how much you change the background. For example, we can use vis to change to a similar background, such as a street. Using vis gives us a clear and crisp background.\n",
    "\n",
    "| Original Video | Street background |\n",
    "|---|---|\n",
    "| ![wave](assets/wave.png) | ![street](assets/street.png) |\n",
    "\n",
    "\n",
    "However, this could backfire when we modify the background to something that has very different visual elements. For example, if we change it to an ocean background, we can see that having vis leaves some artifacts in the background. In this case, we *don't want vis*. This does generate more of a blurry background, but it's better than our original result! \n",
    "\n",
    "\n",
    "| Original Video | With Vis | Without Vis |\n",
    "|---|---|---|\n",
    "| ![wave](assets/wave.png) | ![street](assets/ocean_vis.png) | ![street](assets/ocean_no_vis.png) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48d6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A realistic, static full-body shot of a young man standing outdoors near the coast. He has short dark hair and is dressed casually in a dark grey t-shirt, loose black pants, and white sneakers, with an ID badge clipped to his waistband. He faces the camera directly and waves his right hand continuously in a friendly greeting. The surrounding environment is bright and open. In the background, a vast ocean stretches out toward the horizon, with gentle waves, shimmering reflections, and a clear blue sky above. A coastal walkway with railings and scattered pedestrians lines the foreground, replacing the busy city street elements. Soft natural lighting from the sun enhances the calm, breezy seaside atmosphere.\"\n",
    "with open(\"prompts/prompt_background.txt\", \"w\") as f:\n",
    "    f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CT2.5 \n",
    "# WARNING: This should take a couple of minutes\n",
    "!{sys.executable} cosmos_transfer2_5/examples/inference.py -i scripts/background_change.jsonl -o outputs/background_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c169aae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"600\" controls>\n",
       "  <source src=\"outputs/background_change/background_change.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(f\"\"\"\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"outputs/background_change/background_change.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3104e6",
   "metadata": {},
   "source": [
    "## 4. Generating Realistic Data from Omniverse\n",
    "\n",
    "An important robotics workflow is \"Sim-to-Real.\" NVIDIA Omniverse can generate synthetic data, but we can use CT 2.5 to add real-world domain randomization (new lighting, textures, backgrounds) and generate photorealistic scenes.\n",
    "\n",
    "The Workflow:\n",
    "1. Generate in Omniverse: Create a base scenario (e.g., cars driving around) and export the video.\n",
    "2. Extract Ground Truth: From Omniverse, also export the perfect ground-truth modalities (Depth, Segmentation, Edge).\n",
    "3. Augment with CT 2.5: Use these perfect synthetic controls to run CT 2.5 with a new prompt (e.g., \"in a dimly lit snowy day\").\n",
    "4. Package with Cosmos Writer: Save the new, augmented video alongside the original, ground-truth controls. This teaches a downstream model to associate the ground-truth controls with the new, realistic style.\n",
    "\n",
    "\n",
    "### Omniverse Control Modalities\n",
    "\n",
    "We start with the following control modalities:\n",
    "\n",
    "| Original Video | Edge | Seg | Depth |\n",
    "|----------|----------|----------|----------|\n",
    "| <video src=\"simulation_data/simulator_rgb_input.mp4\" controls width=\"300\"></video> | <video src=\"simulation_data/simulator_edge.mp4\" controls width=\"300\"></video> | <video src=\"simulation_data/simulator_segmentation.mp4\" controls width=\"300\"></video> | <video src=\"simulation_data/simulator_depth.mp4\" controls width=\"300\"></video> |\n",
    "\n",
    "\n",
    "### Recipe \n",
    "\n",
    "\n",
    "| Task | Suggested Controls & Settings| Example Results | Prompt |\n",
    "|--|--|---|-|\n",
    "|Original Video| N/A | <video src=\"TODO\" width=\"100%\" controls></video> | N/A |\n",
    "|Photorealistic Generation|TODO| <video src=\"TODO\" width=\"100%\" controls></video> | [Prompt Location](simulation_data/prompt_location.txt) |\n",
    "|Fog|TODO| <video src=\"TODO\" width=\"100%\" controls></video> | [Prompt Location](simulation_data/fog.txt) |\n",
    "|Morning Sunlight|TODO| <video src=\"TODO\" width=\"100%\" controls></video> | [Prompt Location](simulation_data/morning_sun.txt) |\n",
    "|Night|TODO| <video src=\"TODO\" width=\"100%\" controls></video> | [Prompt Location](simulation_data/night.txt) |\n",
    "|Rain|TODO| <video src=\"TODO\" width=\"100%\" controls></video> | [Prompt Location](simulation_data/rain.txt) |\n",
    "|Snow|TODO| <video src=\"TODO\" width=\"100%\" controls></video> | [Prompt Location](simulation_data/snow.txt) |\n",
    "|Wooden Road|TODO| <video src=\"TODO\" width=\"100%\" controls></video> | [Prompt Location](simulation_data/wooden_road.txt) |\n",
    "\n",
    "<!-- #### Example Results:\n",
    "<div style=\"display: flex; gap: 20px;\">\n",
    "  <video src=\"TODO\" width=\"45%\" controls></video>\n",
    "  <video src=\"TODO\" width=\"45%\" controls></video>\n",
    "</div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9f2c1",
   "metadata": {},
   "source": [
    "### Photorealistic Generation\n",
    "Let us change our simulated environment to look realistic\n",
    "```json\n",
    "{\n",
    "  \"name\": \"omniverse_photorealistic\",\n",
    "  \"prompt_path\": \"prompt.txt\",\n",
    "  \"video_path\": \"original.mp4\",\n",
    "  \"guidance\": 7,\n",
    "  \"edge\": {\n",
    "    \"control_weight\": 1.0,\n",
    "    \"control_path\": \"edges.mp4\"\n",
    "  },\n",
    "  \"seg\": {\n",
    "    \"control_weight\": 0.9,\n",
    "    \"control_path\": \"segmentation.mp4\",\n",
    "    \"mask_prompt\": \"battered orange safety cone\"​\n",
    "  },\n",
    "  \"depth\": {\n",
    "    \"control_weight\": 0.9,\n",
    "    \"control_path\": \"segmentation.mp4\",\n",
    "    \"mask_path\": \"mask_inverted.mp4\"\n",
    "  }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93737e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run CT2.5 WARNING: This should take a couple of minutes\n",
    "# Running the script:\n",
    "    #- For single GPU: python examples/inference.py -i scripts/omniverse_av_configs.jsonl -o outputs/omniverse_generations_av\n",
    "    #- For Multi GPU: torchrun --nproc_per_node=8 --master_port=12341 examples/inference.py -i scripts/omniverse_av_configs.jsonl -o outputs/omniverse_generations_av\n",
    "\n",
    "torchrun --nproc_per_node=8 --master_port=12341 examples/inference.py -i scripts/omniverse_av_configs.jsonl -o outputs/omniverse_generations_av"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c6c3d",
   "metadata": {},
   "source": [
    "## Prompt Generator for Scene Conditions\n",
    "This module provides a configurable system for automatically generating natural-language prompts based on selected environmental, weather, and road-surface conditions. It is designed for data generation, augmentation workflows, or any pipeline where you want consistent, high-quality scene descriptions without manually rewriting prompts.\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "The system uses:\n",
    "- A SceneConfig dataclass\n",
    "- Three condition dictionaries:\n",
    "    - ENV_LIGHTING\n",
    "    - WEATHER\n",
    "    - ROAD_SURFACE\n",
    "- A single function: generate_prompt(config)\n",
    "\n",
    "It takes your base scene, inserts the selected conditions, and returns a polished final prompt. \n",
    "\n",
    "#### Code Structure:\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List\n",
    "\n",
    "ENV_LIGHTING = { ... }\n",
    "WEATHER = { ... }\n",
    "ROAD_SURFACE = { ... }\n",
    "\n",
    "@dataclass\n",
    "class SceneConfig:\n",
    "    base_scene: str\n",
    "    env_lighting: Optional[str] = None\n",
    "    weather: Optional[str] = None\n",
    "    road_surface: Optional[str] = None\n",
    "    extra_tags: Optional[List[str]] = None\n",
    "\n",
    "def generate_prompt(config: SceneConfig) -> str:\n",
    "    parts = [config.base_scene.strip()]\n",
    "    if config.env_lighting: parts.append(f\"The scene is {ENV_LIGHTING[config.env_lighting]}.\")\n",
    "    if config.weather: parts.append(WEATHER[config.weather])\n",
    "    if config.road_surface: parts.append(ROAD_SURFACE[config.road_surface])\n",
    "    parts.append(\"All visual elements should be consistent with these conditions.\")\n",
    "    return \" \".join(p for p in parts if p)\n",
    "```\n",
    "\n",
    "\n",
    "You can find the full codebase at [src/prompt_generation.py](src/prompt_generation.py)\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "config = SceneConfig(\n",
    "    base_scene=\"A busy urban intersection with multiple vehicles.\",\n",
    "    env_lighting=\"sunrise\",\n",
    "    weather=\"fog\",\n",
    "    road_surface=\"wooden\"\n",
    ")\n",
    "\n",
    "print(generate_prompt(config))\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "A busy urban intersection with multiple vehicles.\n",
    "The scene is bathed in warm morning light.\n",
    "A layer of fog softens distant structures.\n",
    "The road surface is made of wooden planks.\n",
    "All visual elements should be consistent with these conditions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad6042d",
   "metadata": {},
   "source": [
    "## Additional Recipes\n",
    "Didn't find something you were looking for? There's a bunch of examples in the [cosmos cookbook](https://nvidia-cosmos.github.io/cosmos-cookbook/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1cca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
